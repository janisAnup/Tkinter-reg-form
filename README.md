Tkinter Registration Form

ðŸŽ¯ A simple and stylish registration form built using Python's Tkinter library.

ðŸ› ï¸ Features:
- User-friendly GUI
- Entry fields for registration details
- Styled using a separate `designs.py` file for cleaner code
- Modular structure

ðŸ“‚ Files:
- `main.py` â€” main application logic
- `designs.py` â€” holds all the styling for the form
- `README.md` â€” you're reading it ðŸ˜‰

ðŸ‘©â€ðŸ’» Created by Janis Anup  
ðŸ§‘â€ðŸ’¼ Internship Task 1 @ Next24Tech



Assginment-1
 Student REST API
 Repo is available in the https://github.com/christopher-pb location
 1.Clone the code and push it to the master/main branch
 2.Raise a pull request and merge the code to main/master 
 3. Should be able to build the code and should be shown in the action tab in github as successful
 
 Assginment-2
 Employee REST API
 Repo is available in the https://github.com/christopher-pb location and go through the README.md file
 1.Clone the code and do the same for employee with json itself and not database
	Employees
		EmployeeID 
		FirstName
		LastName
		Gender
		DateOfBirth
	Department Table

	Departments
		DepartmentID 
		DepartmentName
		Location
	Salaries
		SalaryID 
		EmployeeID 	
		BasicSalary
		Bonus
		Allowances
 2.Raise a pull request and merge the code to main/master 
 3. Should be able to build the code and should be shown in the action tab in github as successful
 4.Download the code and run the application 
 5.Use the postman to check the rest call





 UNIT 1 â€” ALL QUESTIONS WITH ANSWERS
2-MARK QUESTIONS (80 words)
Q1. What is NLP?
Answer: Natural Language Processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with interactions between computers and human languages. It involves a computer extracting meaningful information from natural language input and producing natural language output. NLP is the technology used by machines to understand, analyse, manipulate, and interpret human languages. It helps developers organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition, speech recognition, relationship extraction, and topic segmentation.

Q2. What are the two components of NLP?
Answer: The two components are: (1) Natural Language Understanding (NLU) â€” helps machines understand and analyse human language by extracting metadata from content such as concepts, entities, keywords, emotion, relations, and semantic roles. It maps given input into useful representations and analyses different aspects of language. (2) Natural Language Generation (NLG) â€” acts as a translator converting computerized data into natural language representation. It involves Text Planning (what to say), Sentence Planning (how to say it), and Text Realization (producing actual text).

Q3. What was the Georgetown-IBM experiment?
Answer: The Georgetown-IBM experiment was conducted in 1954 and was one of the earliest demonstrations of machine translation. It involved automatic translation of 60 Russian sentences into English using a simple system with only 6 grammar rules and 250 vocabulary words. Scientists believed full machine translation would be achieved within 3-5 years, but this proved overly optimistic. It was a landmark event in the history of NLP that sparked significant interest and funding in machine translation research during the 1950s.

Q4. What is the Turing Test?
Answer: The Turing Test was proposed by Alan Turing in 1950 in his paper "Computing Machinery and Intelligence." It tests whether a machine can exhibit intelligent behaviour indistinguishable from a human. A human judge conducts text-based conversations with both a machine and a human without knowing which is which. If the judge cannot reliably distinguish the machine from the human, the machine passes the test. It remains a foundational concept in artificial intelligence and NLP for measuring machine intelligence.

Q5. What was SHRDLU?
Answer: SHRDLU was a program written by Terry Winograd in 1968-70 that helped users communicate with a computer and move objects in a virtual world. It could handle instructions such as "pick up the green ball" and answer questions like "What is inside the black box." The main importance of SHRDLU was that it showed that syntax, semantics, and reasoning about the world could be combined to produce a system that understands natural language. However, it only worked within its limited blocks world.

Q6. What was the LUNAR system?
Answer: LUNAR was the classic example of a Natural Language database interface system that used Augmented Transition Networks (ATNs) and Woods' Procedural Semantics. It was designed to answer questions about lunar rock samples collected during Apollo missions. LUNAR was capable of translating elaborate natural language expressions into database queries and could handle 78% of requests without errors. It demonstrated that computers could understand complex English questions and automatically convert them into structured queries for information retrieval.

Q7. What is Case Grammar?
Answer: Case Grammar was developed by linguist Charles J. Fillmore in 1968. It uses languages such as English to express the relationship between nouns and verbs by using prepositions. In Case Grammar, case roles are defined to link certain kinds of verbs and objects. For example, in "Neha broke the mirror with the hammer": Neha is identified as the agent (who performed the action), mirror as the theme (what was affected), and hammer as the instrument (the tool used to perform the action).

Q8. What is an Augmented Transition Network (ATN)?
Answer: An Augmented Transition Network (ATN) is a finite state machine that is capable of recognizing regular languages. It was a key development during the 1960-1980 period of NLP. ATNs extend basic Finite State Automata by adding memory and register capabilities, allowing the machine to store and recall information while processing input. This enabled parsing of more complex sentence structures than simple FSAs could handle. ATNs were used in systems like LUNAR for parsing natural language queries into database operations.

Q9. What is Chomsky's Generative Grammar?
Answer: In 1957, Noam Chomsky introduced the idea of Generative Grammar in his book "Syntactic Structures." He claimed that language is generative in nature â€” a finite set of rules can generate an infinite number of valid sentences. Generative Grammar provides rule-based descriptions of syntactic structures. For example, the rule S â†’ NP VP states a sentence consists of a noun phrase followed by a verb phrase. This framework became foundational for computational linguistics, NLP, and the development of grammar-based language models.

Q10. List four advantages of NLP.
Answer: (1) NLP helps users ask questions about any subject and get a direct response within seconds. (2) NLP offers exact answers to the question â€” it does not offer unnecessary and unwanted information. (3) NLP helps computers communicate with humans in their languages, making technology accessible. (4) Most companies use NLP to improve the efficiency of documentation processes, accuracy of documentation, and identify the information from large databases. Additionally, NLP is very time efficient in processing large volumes of text data.

Q11. List the disadvantages of NLP.
Answer: (1) NLP may not show context â€” it struggles to understand implied meaning, sarcasm, and cultural nuances. (2) NLP is unpredictable â€” it may produce incorrect or inconsistent outputs for similar inputs. (3) NLP may require more keystrokes compared to simple keyword-based search approaches. (4) NLP is unable to adapt to the new domain, and it has a limited function â€” it is built for a single and specific task only, requiring significant retraining or redesign for different applications and domains.

Q12. What is Language Modeling?
Answer: Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. LM is used in artificial intelligence, NLP, natural language understanding and generation systems, particularly those performing text generation, machine translation, and question answering. For example, given "I want to", a language model assigns probabilities to next words like "eat" or "go."

Q13. What is an N-gram language model?
Answer: An N-gram model creates a probability distribution for a sequence of N words. The N defines the size of the gram or sequence of words being assigned a probability. This allows the model to predict the next word in a sentence. For example, if N=5, a gram might look like "can you please call me." The model calculates P(next word | previous N-1 words) using counts from training data. N-gram models are simple but effective for autocomplete, spell correction, and speech recognition tasks.

Q14. What is a Unigram model?
Answer: A Unigram is the simplest type of language model where N=1. It doesn't look at any conditioning context in its calculations. It evaluates each word or term independently. The probability of a sentence is the product of individual word probabilities: P("the cat sat") = P("the") Ã— P("cat") Ã— P("sat"). Since unigrams ignore word order, "the cat sat" and "sat cat the" have identical probabilities. Despite this limitation, unigrams serve as useful baselines and are used in basic information retrieval tasks.

Q15. What is a Bidirectional language model?
Answer: Unlike N-gram models which analyze text in one direction (backward â€” considering only previous words), bidirectional models analyze text in both directions â€” backward and forward simultaneously. This means they consider both preceding and following words to make predictions. For example, in "The ___ sat on the mat," a bidirectional model uses "The" (before) AND "sat on the mat" (after) to predict the missing word. BERT (Bidirectional Encoder Representations from Transformers), introduced by Google in 2018, is the most well-known bidirectional language model.

Q16. What are Neural Language Models?
Answer: Neural language models use deep learning techniques to overcome the limitations of N-gram models. These models use neural networks such as Recurrent Neural Networks (RNNs) and Transformers to capture complex patterns and dependencies in text. Unlike N-grams which have fixed and limited context windows, neural models can learn long-range relationships between words across entire sentences or documents. Examples include GPT models (using Transformer architecture for text generation) and BERT (bidirectional understanding), achieving near-human performance in language tasks.

Q17. Name four uses of Language Modeling.
Answer: (1) Speech recognition â€” machines process speech audio; voice assistants like Siri and Alexa use speech recognition powered by language models. (2) Text generation â€” uses prediction to generate coherent and contextually relevant text for creative writing and summarization. (3) Chatbots â€” engage in humanlike conversations and generate accurate responses for customer support and information retrieval. (4) Machine translation â€” translation of one language to another by programs like Google Translate and Microsoft Translator, which rely on language models for fluent output.

Q18. What is a Regular Expression?
Answer: A Regular Expression (RegEx) is a sequence of characters mainly used to find or replace patterns embedded in text. It is a set of characters or pattern used to find substrings in a given string. A regular expression is a language for specifying text search strings that helps match or extract strings using specialized syntax. The Regular Expression language was formalized by American mathematician Stephen Cole Kleene. In NLP, regex is used for data cleaning, named entity recognition (extracting emails, phone numbers), and text normalization.

Q19. How do Regular Expressions work? Explain with the student name example.
Answer: Consider students: Sunil, Shyam, Ankit, Surjeet, Sumit, Subhi, Surbhi, Siddharth, Sujan. The pattern requires first two letters as S and u, followed by exactly three positions taken by any letter. Checking: Sunil (S-u-n-i-l âœ…), Shyam (second letter 'h' not 'u' âŒ), Ankit (starts with 'A' âŒ), Surjeet (too many letters after Su âŒ), Sumit (S-u-m-i-t âœ…), Subhi (S-u-b-h-i âœ…), Surbhi (too long âŒ), Siddharth (Si not Su âŒ), Sujan (S-u-j-a-n âœ…). Extracted: Sunil, Sumit, Subhi, Sujan.

Q20. State the mathematical properties of Regular Expressions.
Answer: (1) Îµ is a Regular Expression indicating the language has an empty string. (2) Ï† is a Regular Expression denoting an empty language. (3) If X and Y are Regular Expressions, then X.Y (concatenation of XY), X+Y (union of X and Y), and X*, Y* (Kleene Closure of X and Y) are also regular expressions. (4) If a string is derived from the above rules, then that would also be a regular expression. These mathematical properties form the algebraic foundation for all pattern matching operations.

Q21. Name three NLP uses of Regular Expressions.
Answer: (1) Data Cleaning and Pre-processing â€” regex removes unwanted characters such as special characters, punctuation, and numbers that can interfere with NLP algorithms and models. (2) Named Entity Recognition (NER) â€” regex extracts named entities such as names, locations, organizations, email IDs, and phone numbers from large unstructured text content. (3) Text Normalization â€” regex transforms text data into standard format, such as converting all text to lowercase or removing stop words, making NLP tasks more accurate and effective.

Q22. Define Finite State Automaton formally.
Answer: The term automata derives from Greek "Î±á½Ï„ÏŒÎ¼Î±Ï„Î±" meaning "self-acting." An automaton is an abstract self-propelled computing device following a predetermined sequence of operations automatically. An automaton with finite states is called Finite Automaton (FA) or Finite State Automata (FSA). Mathematically represented as 5-tuple (Q, Î£, Î´, qâ‚€, F): Q is a finite set of states, Î£ is the alphabet, Î´ is the transition function, qâ‚€ is the initial state from where input is processed (qâ‚€ âˆˆ Q), F is the set of final states (F âŠ† Q).

Q23. What is Morphological Analysis?
Answer: Morphological analysis is a field of linguistics that studies the structure of words. It identifies how a word is produced through the use of morphemes. A morpheme is the basic unit of the English language â€” the smallest element of a word that has grammatical function and meaning. Morphology studies how words are built from smaller meaning-bearing units. Two broad classes of morphemes exist: stems (the main morpheme supplying the core meaning) and affixes (adding additional meaning of various kinds including prefixes, suffixes, infixes, and circumfixes).

Q24. What are the types of affixes? Give examples.
Answer: Affixes are bound morphemes adding additional meaning to stems. Four types: (1) Suffix â€” added after root: eatâ†’eats, walkâ†’walked. (2) Prefix â€” added before root: unâ†’un-buckle, reâ†’re-write. (3) Circumfix â€” added around root on both sides: German "sagen" (to say) â†’ "ge-sag-t" (said), with ge- before and -t after. (4) Infix â€” inserted inside root: Tagalog "hingi" (borrow) â†’ "humingi" (the agent of an action), with -um- inserted within the word.

Q25. What is a Lexicon in NLP?
Answer: A lexicon is the vocabulary â€” the body of words used in a particular language or subject. In morphological analysis, the lexicon transducer maps between the lexical level (with its stems and morphological features) and an intermediate level that represents a simple concatenation of morphemes. Then a host of transducers, each representing a single spelling rule constraint, all run in parallel to map between this intermediate level and the surface level (actual written form). The lexicon serves as the dictionary validating root words during morphological processing.

Q26. What is Tokenization?
Answer: Tokenization, in the realm of Natural Language Processing and machine learning, refers to the process of converting a sequence of text into smaller parts known as tokens. These tokens can be as small as characters or as long as words. For example, "Chatbots are helpful" â†’ word tokens: ["Chatbots", "are", "helpful"], or character tokens: ["C","h","a","t","b","o","t","s"," ","a","r","e"," ","h","e","l","p","f","u","l"]. It is the fundamental first step in almost every NLP pipeline.

Q27. What are the types of Tokenization?
Answer: (1) Word tokenization â€” breaks text into individual words; most common for English with clear word boundaries. (2) Character tokenization â€” segments text into individual characters; beneficial for languages lacking clear word boundaries or tasks requiring granular analysis like spelling correction. (3) Subword tokenization â€” strikes a balance between word and character tokenization, breaking text into units larger than a character but smaller than a word. For example, "Chatbots" â†’ ["Chat", "bots"]. Useful for out-of-vocabulary words in NLP tasks.

Q28. Name three use cases of Tokenization.
Answer: (1) Search engines â€” when you type a query into Google, it employs tokenization to dissect your input, helping the engine sift through billions of documents to present the most relevant results. (2) Machine translation â€” tools like Google Translate utilize tokenization to segment sentences in the source language; tokenized segments are translated and reconstructed in the target language retaining original context. (3) Speech recognition â€” voice assistants like Siri or Alexa convert spoken words to text, then tokenize to process requests.

Q29. What is Spell Checking in NLP?
Answer: Spell checking is the process of identifying words in texts that have spelling errors or are misspelled. Text data from social media or extracted from images using Optical Character Recognition (OCR) usually contains typos, misspellings, spurious symbols, or errors that impact machine learning models. For example, if "John" is present both correctly and as "J0hn" (zero replacing 'o'), a model treats them as two separated words causing unexpected prediction outcomes. Spell checking detects and corrects such errors to improve data quality.

Q30. Define Minimum Edit Distance.
Answer: The Minimum Edit Distance between two strings str1 and str2 is defined as the minimum number of insert/delete/substitute operations required to transform str1 into str2. For example, str1="ab", str2="abc" â€” inserting character 'c' on str1 transforms it into str2, so edit distance is 1. Edit distance can also be calculated as operations to transform str2 into str1 â€” deleting 'c' from "abc" gives "ab" with same distance of 1. The word "giraffe" differs from "graffe" by only one letter, making it the closest match.

Q31. Give an example of Minimum Edit Distance.
Answer: For str1="INTENTION" and str2="EXECUTION", the minimum edit distance is 5. All operations are performed on str1. The algorithm uses a dynamic programming matrix of size (10Ã—10). Each cell is computed as the minimum of: cell above + 1 (deletion), cell to the left + 1 (insertion), and diagonal cell + substitution cost (0 if characters match, 2 if different). The bottom-right cell of the completed matrix gives the final answer. Applications include spell correction where candidates are ranked by their edit distance from the misspelled word.

Q32. What is an Exponential Language Model?
Answer: An Exponential Language Model is based on the principle of entropy, which is defined as a state of disorder, randomness, or uncertainty. The model states that the probability distribution with the most entropy is the best choice. This means the model avoids making unjustified assumptions about the data â€” it selects the most uniform distribution that still satisfies known constraints from training data. Exponential models are also called Maximum Entropy models and are used in NLP tasks like part-of-speech tagging and text classification.

Q33. What are Parts-of-Speech tagging and Parsing as language model uses?
Answer: Parts-of-Speech tagging involves the markup and categorization of words by certain grammatical characteristics (noun, verb, adjective, etc.). It was first used in the study of the Brown Corpus â€” a body of random English prose designed to be studied by computers. This corpus trained several important language models, including one used by Google to improve search quality. Parsing involves analysis of any string of data or sentence that conforms to formal grammar and syntax rules, depicting each word's relationship to others. Spell-checking applications use parsing.

5-MARK QUESTIONS (150 words)
Q1. Explain the origin and history of NLP from 1940s to the present.
Answer:

1940-1960 (Machine Translation era): NLP started in the 1940s. In 1948, the first recognizable NLP application was introduced at Birkbeck College, London. In the 1950s, there was a conflicting view between linguistics and computer science. Chomsky developed his first book "Syntactic Structures" and claimed language is generative in nature. In 1957, he introduced Generative Grammar â€” rule-based descriptions of syntactic structures.

1960-1980 (AI era): Key developments included Augmented Transition Networks (ATNs) â€” finite state machines capable of recognizing regular languages. Charles Fillmore developed Case Grammar (1968) to express noun-verb relationships using case roles like agent, theme, and instrument. Key systems: SHRDLU (1968-70) demonstrated natural language understanding in a virtual blocks world, handling instructions and questions by combining syntax, semantics, and reasoning. LUNAR provided natural language database querying using ATNs and Procedural Semantics, handling 78% of requests without errors.

Modern Era: Modern NLP consists of speech recognition, machine translation, and machine text reading. Combined, these allow AI to gain knowledge of the world, powering applications like Amazon Alexa where users can ask questions and receive spoken answers.

Q2. Explain NLU and NLG as components of NLP in detail.
Answer:

Natural Language Understanding (NLU) helps the machine understand and analyse human language by extracting metadata from content such as concepts, entities, keywords, emotion, relations, and semantic roles. NLU is mainly used in business applications to understand the customer's problem in both spoken and written language. It involves two key tasks: (1) mapping given input into useful representations that computers can process, and (2) analyzing different aspects of the language including grammar, meaning, and context. For example, when a customer writes "My order is late and I'm angry," NLU extracts entity (order), emotion (anger), and intent (complaint).

Natural Language Generation (NLG) acts as a translator that converts computerized data into natural language representation. It involves three stages: (1) Text Planning â€” determining what content to communicate by selecting relevant information; (2) Sentence Planning â€” deciding sentence structure, choosing appropriate words and syntax; (3) Text Realization â€” producing the final grammatically correct and fluent text output. For example, given structured data about weather (temperature=35Â°C, humidity=80%), NLG generates: "Expect a hot and humid day."

Together NLU (input) and NLG (output) form the complete NLP cycle enabling full human-computer communication.

Q3. Explain the advantages and disadvantages of NLP.
Answer:

Advantages: (1) NLP helps users ask questions about any subject and get a direct response within seconds, enabling rapid access to information from vast data sources. (2) NLP offers exact answers to the question â€” it does not offer unnecessary and unwanted information, improving search precision. (3) NLP helps computers communicate with humans in their languages, making technology accessible to non-technical users globally. (4) It is very time efficient â€” processing thousands of documents in minutes rather than hours of manual reading. (5) Most companies use NLP to improve the efficiency of documentation processes, accuracy of documentation, and identify important information from large databases, automating formerly manual workflows.

Disadvantages: (1) NLP may not show context â€” it struggles with implied meaning, sarcasm, cultural references, and situational nuances in language. (2) NLP is unpredictable â€” similar inputs may produce inconsistent or incorrect outputs. (3) NLP may require more keystrokes than simple keyword-based search methods. (4) NLP is unable to adapt to the new domain, and it has a limited function â€” it is built for a single and specific task only. Switching to a new domain requires significant retraining or complete redesign of the system.

Q4. Explain the different types of Language Models with examples and uses.
Answer:

N-gram Model: Creates probability distributions for sequences of N words, predicting the next word based on previous N-1 words. Example: N=5 gram "can you please call me." Simple and effective for autocomplete.

Unigram Model: Simplest type (N=1) â€” evaluates each word independently without considering context. P("cat sat") = P("cat") Ã— P("sat"). Ignores word order entirely.

Bidirectional Model: Analyzes text in both directions â€” backward and forward â€” unlike N-grams which only look backward. Captures context from both sides of each word. BERT is the leading example.

Exponential Model: Based on the principle of entropy (state of disorder, randomness, uncertainty). Selects the probability distribution with the most entropy as the best choice, avoiding unjustified assumptions.

Neural Language Models: Use deep learning â€” RNNs and Transformers â€” to capture complex patterns and long-range dependencies in text, overcoming N-gram limitations.

Key Uses: (1) Speech recognition (Siri, Alexa). (2) Text generation for creative writing and summarization. (3) Chatbots for customer support. (4) Machine translation (Google Translate, Microsoft Translator). (5) Parts-of-speech tagging â€” categorizing words grammatically, famously used in the Brown Corpus study and by Google for search improvement. (6) Parsing â€” analyzing sentence structure, used in spell-checking applications.

Q5. What are Regular Expressions? Explain properties, working, and NLP applications.
Answer:

Definition: Regular Expressions (RegEx) are sequences of characters used to find or replace patterns in text. A regex is a language for specifying text search strings, formalized by mathematician Stephen Cole Kleene. It helps match or extract strings using specialized syntax â€” for example, extracting all email IDs or phone numbers from large unstructured text.

Mathematical Properties: (1) Îµ is a RegEx indicating empty string. (2) Ï† is a RegEx denoting empty language. (3) If X and Y are RegEx, then X.Y (concatenation), X+Y (union), and X*, Y* (Kleene Closure) are also regular expressions. (4) Any string derived from these rules is also a regular expression.

Working Example: Students: Sunil, Shyam, Ankit, Surjeet, Sumit, Subhi, Surbhi, Siddharth, Sujan. Pattern: names with first two letters S and u, followed by exactly 3 characters. Checking each name against the pattern: Sunil âœ…, Shyam âŒ (not Su), Ankit âŒ, Surjeet âŒ (too long), Sumit âœ…, Subhi âœ…, Surbhi âŒ (too long), Siddharth âŒ, Sujan âœ…. Extracted: Sunil, Sumit, Subhi, Sujan.

NLP Applications: (1) Data cleaning â€” removing special characters and punctuation. (2) Named Entity Recognition â€” extracting names, locations, organizations. (3) Text normalization â€” converting text to standard format.

Q6. Explain Finite State Automata â€” definition, formal representation, and components.
Answer:

The term "automata" derives from the Greek word "Î±á½Ï„ÏŒÎ¼Î±Ï„Î±" meaning "self-acting" and is the plural of automaton â€” an abstract self-propelled computing device that follows a predetermined sequence of operations automatically. An automaton having a finite number of states is called a Finite Automaton (FA) or Finite State Automata (FSA).

Formal Representation â€” 5-tuple (Q, Î£, Î´, qâ‚€, F):

Q is a finite set of states â€” the different positions the machine can be in, represented as circles in diagrams. Î£ is a finite set of symbols called the alphabet of the automaton â€” the valid input characters the machine can read. Î´ is the transition function â€” defines how the machine moves from one state to another when reading an input symbol, represented as arrows between states. qâ‚€ is the initial state from where any input is processed (qâ‚€ âˆˆ Q) â€” the starting point of computation. F is a set of final state/states of Q (F âŠ† Q) â€” if the machine ends in one of these states after reading all input, the string is accepted.

Example: FSA accepting "cat": Q={qâ‚€,qâ‚,qâ‚‚,qâ‚ƒ}, Î£={a-z}, Î´(qâ‚€,'c')=qâ‚, Î´(qâ‚,'a')=qâ‚‚, Î´(qâ‚‚,'t')=qâ‚ƒ, initial state=qâ‚€, F={qâ‚ƒ}. Input "cat" traces qâ‚€â†’qâ‚â†’qâ‚‚â†’qâ‚ƒ (accepted). Input "car" reaches qâ‚‚ with 'r' where no transition exists (rejected).

Q7. Explain English Morphology â€” morphemes, stems, affixes with all four types.
Answer:

Morphological analysis studies the structure of words, identifying how words are produced through morphemes â€” the basic unit of language and the smallest element with grammatical function and meaning. Morphology studies how words are built from smaller meaning-bearing units.

Two Broad Classes: (1) Stems â€” the "main" morpheme of the word, supplying the main meaning (cat, run, happy). (2) Affixes â€” add "additional" meaning of various kinds and are further divided into four types.

Suffix (added after root) â€” most common in English: eatâ†’eats (third person), walkâ†’walked (past tense), happiâ†’happiness (adjective to noun).

Prefix (added before root): un-buckle (reversal of action), re-write (repetition), dis-agree (opposition).

Circumfix (added around root â€” both sides simultaneously): German "sagen" (to say) â†’ "ge-sag-t" (said). The prefix "ge-" and suffix "-t" are added together, neither alone carries the past tense meaning.

Infix (inserted inside root word): In Philippine language Tagalog, "hingi" (borrow) â†’ "humingi" (the agent of an action). The morpheme "-um-" is inserted within the root.

NLP Importance: Morphological analysis enables search systems to match word variants, reduces vocabulary for ML models, and helps machine translation correctly handle tense and grammatical relationships.

Q8. Explain the Lexicon and Transducer system for morphological analysis.
Answer:

A lexicon is the vocabulary â€” the body of words used in a particular language or subject. In NLP morphological analysis, it stores all valid root words with their grammatical categories.

The lexicon transducer maps between the lexical level (with its stems and morphological features, e.g., "fox +Noun +Plural") and an intermediate level that represents a simple concatenation of morphemes (e.g., "fox+s"). Then a host of transducers, each representing a single spelling rule constraint, all run in parallel so as to map between this intermediate level and the surface level (the actual written form, e.g., "foxes").

Three-Level Process:

Lexical level: fox +Noun +Plural (dictionary representation)
Intermediate level: fox + s (morphemes concatenated)
Surface level: foxes (actual spelling after rules applied)
Spelling Rule Examples: (1) Nouns ending in x, s, sh, ch take "-es" instead of "-s": foxâ†’foxes, busâ†’buses. (2) Nouns ending in consonant+y change y to ies: babyâ†’babies. (3) Regular nouns just add "-s": catâ†’cats.

Analysis direction: surface "foxes" â†’ spelling rules identify "es" after "x" = plural â†’ lexicon confirms "fox" is a valid noun â†’ output: "fox +Noun +Plural." Generation direction: "fox +Noun +Plural" â†’ lexicon validates â†’ spelling rule applies "-es" â†’ output: "foxes."

Q9. Explain Tokenization â€” definition, types, and use cases in detail.
Answer:

Definition: Tokenization, in the realm of NLP and machine learning, refers to the process of converting a sequence of text into smaller parts known as tokens. These tokens can be as small as characters or as long as words. Example: "Chatbots are helpful" â†’ ["Chatbots", "are", "helpful"] (word tokens) or ["C","h","a","t","b","o","t","s"," ","a","r","e"," ","h","e","l","p","f","u","l"] (character tokens).

Types: (1) Word tokenization â€” breaks text into individual words. Most common approach, particularly effective for languages with clear word boundaries like English. (2) Character tokenization â€” segments text into individual characters. Beneficial for languages lacking clear word boundaries or for tasks requiring granular analysis such as spelling correction. (3) Subword tokenization â€” strikes a balance between word and character tokenization, breaking text into units larger than a single character but smaller than a full word. For example, "Chatbots" â†’ ["Chat", "bots"]. Especially useful for handling out-of-vocabulary words in NLP tasks.

Use Cases: (1) Search engines â€” Google employs tokenization to dissect user queries, helping sift through billions of documents for relevant results. (2) Machine translation â€” Google Translate tokenizes sentences in the source language, translates segments, and reconstructs them in the target language retaining context. (3) Speech recognition â€” Siri and Alexa convert spoken words to text, then tokenize to process and act upon user requests.

Q10. Explain the process of detecting and correcting spelling errors in NLP.
Answer:

Importance: Spell checking identifies misspelled words in texts. Data from social media or extracted using OCR usually contains typos, misspellings, spurious symbols, or errors that impact machine learning models. For example, if "John" appears both correctly and as "J0hn" (zero replacing 'o'), a model treats them as two separated words, causing unexpected outcomes in predictions.

Detection: Non-word errors (e.g., "graffe") are detected by dictionary lookup â€” the word doesn't exist. Real-word errors (e.g., "there" vs "their") require context analysis since both are valid words.

Correction using Noisy Channel Model: The system assumes the user intended a correct word w, but noise corrupted it into misspelling x. The formula w* = argmax P(x|w) Ã— P(w) combines: the error model P(x|w) measuring how likely this specific typo is, and the language model P(w) measuring how common the correct word is.

Example: User typed "graffe." Candidates: "giraffe" (differs by one letter â€” edit distance 1), "grail" (distance 4), "graf" (distance 2). The word "giraffe" is intuitively more similar than "grail" or "graf" because it has the smallest edit distance. The Noisy Channel Model confirms: P("graffe"|"giraffe") Ã— P("giraffe") yields the highest score, selecting "giraffe" as the correction.

Q11. Explain Minimum Edit Distance â€” definition, operations, algorithm, and example.
Answer:

Definition: Minimum Edit Distance between two strings str1 and str2 is the minimum number of insert/delete/substitute operations required to transform str1 into str2. For str1="ab", str2="abc", inserting 'c' gives distance 1. Equivalently, deleting 'c' from str2 also gives distance 1.

Three Operations: (1) Insertion â€” adding a character: "ab"â†’"abc." (2) Deletion â€” removing a character: "abc"â†’"ab." (3) Substitution â€” replacing one character: "cat"â†’"car."

Algorithm (Dynamic Programming): Build an (m+1)Ã—(n+1) matrix for strings of length m and n. Initialize first row as 0,1,2,...,n (cost of insertions from empty string) and first column as 0,1,2,...,m (cost of deletions). For each cell[i][j]: if characters match, diagonal cost=0; otherwise substitution cost applies. Cell[i][j] = minimum of: cell[i-1][j]+1 (deletion), cell[i][j-1]+1 (insertion), cell[i-1][j-1]+cost (substitution or match). Bottom-right cell gives the answer.

Example: For str1="INTENTION" and str2="EXECUTION", building the 10Ã—10 matrix and filling each cell, the minimum edit distance is 5 (with substitution cost=2). This means 5 single-character operations transform "INTENTION" into "EXECUTION."

Applications: Spell correction (ranking candidates by distance â€” "giraffe" with distance 1 from "graffe" beats "grail" with distance 4), DNA sequence alignment in bioinformatics, plagiarism detection, and machine translation evaluation using Word Error Rate.

Q12. Explain Grammar-Based and Statistical Language Models in detail.
Answer:

Grammar-Based Language Models define language using formal grammar rules written by human linguists. Rules describe how sentences are structured: S â†’ NP VP (sentence = noun phrase + verb phrase), NP â†’ Det N (noun phrase = determiner + noun). Given rules with terminals like Detâ†’"the"|"a", Nâ†’"cat"|"fish", Vâ†’"ate", the model generates valid sentences like "the cat ate the fish" and rejects invalid orders like "cat the ate fish the." Each rule carries a probability, and a sentence's probability is the product of all applied rule probabilities. Advantages: linguistically motivated and captures grammar structure. Disadvantages: requires expert linguists, cannot cover all variations, rigid with informal language.

Statistical Language Models learn word patterns from large text corpora using probabilistic techniques. The N-gram model predicts the next word using previous N-1 words via the Markov assumption. Bigram calculation: P("cat"|"the") = Count("the cat") / Count("the"). If "the cat" appears 200 times and "the" appears 1000 times in training data, P("cat"|"the") = 0.2. The chain rule computes sentence probability: P(wâ‚wâ‚‚...wâ‚™) = P(wâ‚) Ã— P(wâ‚‚|wâ‚) Ã— P(wâ‚ƒ|wâ‚‚) Ã— ... Advantages: automatic learning from data, handles real-world language, scalable. Disadvantages: zero-probability for unseen sequences, limited context.

Key Difference: Grammar-based models are human-designed (top-down rules), while statistical models are machine-learned (bottom-up from data).

Q13. Explain SHRDLU and LUNAR systems and their significance in NLP history.
Answer:

SHRDLU was a program written by Terry Winograd in 1968-70 at MIT. It enabled users to communicate with a computer and move objects in a virtual "blocks world" â€” a simplified environment containing colored blocks on a table. SHRDLU could handle instructions such as "pick up the green ball" and answer questions like "What is inside the black box." The main importance of SHRDLU was that it demonstrated that syntax (grammar), semantics (meaning), and reasoning about the world could be combined to produce a system that understands natural language. However, its limitation was that it only worked within its tiny virtual blocks world and could not understand language about the real world.

LUNAR was the classic example of a Natural Language database interface system. It used Augmented Transition Networks (ATNs) and Woods' Procedural Semantics to process queries about lunar rock samples from Apollo missions. LUNAR was capable of translating elaborate natural language expressions into database queries. Users could ask complex questions in plain English, and the system would automatically convert them into structured database operations. It achieved 78% accuracy â€” handling 78% of requests without errors.

Significance: Both systems were built during the 1960-1980 AI era of NLP. SHRDLU proved that combining multiple linguistic levels enables language understanding, while LUNAR demonstrated practical natural language interfaces for databases. Together, they showed NLP's potential but also revealed that scaling beyond limited domains remained a major challenge, motivating the shift to statistical approaches in the 1990s.*__


# Unit 2: Word Level Analysis â€” Complete Explanation

---

## 1. N-gram Model

### What is an N-gram?

An **N-gram** is a contiguous sequence of N items (words, characters, or symbols) extracted from text. It's the foundation of statistical language modeling.

```
N = 1 â†’ Unigrams:   "I", "love", "programming"
N = 2 â†’ Bigrams:    "I love", "love programming", "programming I"
N = 3 â†’ Trigrams:   "I love programming", "love programming I"
N = 4 â†’ 4-grams:    "I love programming I"
```

### How N-gram Language Models Work

An N-gram model predicts the next word based on the previous N-1 words using **conditional probability**:

```
P(word | previous N-1 words) = Count(previous N-1 words + word) / Count(previous N-1 words)

Example (Bigram):
P("programming" | "love") = Count("love programming") / Count("love")
```

### Complete Example

**Step 1 â€” Given Text:**
```
"I love programming. I love Python programming."
```

**Step 2 â€” Tokenization:**
```
["I", "love", "programming", "I", "love", "Python", "programming"]
```

**Step 3 â€” Construct N-grams:**
```
Unigrams:  ["I", "love", "programming", "I", "love", "Python", "programming"]
Bigrams:   ["I love", "love programming", "programming I", "I love", "love Python", "Python programming"]
Trigrams:  ["I love programming", "love programming I", "programming I love", "I love Python", "love Python programming"]
```

**Step 4 â€” Frequency Counts (Bigrams):**
```
"I love"            â†’ 2
"love programming"  â†’ 1
"programming I"     â†’ 1
"love Python"       â†’ 1
"Python programming" â†’ 1
```

**Step 5 â€” Calculate Probabilities:**
```
P("programming" | "love") = 1 / (2 + 1 + 1) = 1/4 = 0.25
P("I" | "programming") = 1 / 1 = 1.0
P("Python" | "love") = 1 / 4 = 0.25
```

### Limitations of Unsmoothed N-gram Models

1. **Data Sparsity:** Unseen word sequences get zero probability
   - If "love mathematics" never appeared, P = 0, even though it's valid

2. **High Memory Requirements:** Large N values need massive storage
   - Trigram model needs to store all 3-word combinations

3. **Poor Performance for Rare Sequences:** Low-frequency words get unreliable estimates

4. **Limited Context Understanding:** Only considers N-1 previous words
   - Bigram sees only 1 word back, misses longer dependencies

5. **Sensitivity to Training Corpus:** Performance depends heavily on training data quality and size

---

## 2. Evaluating N-grams

### Evaluation Methods

**Extrinsic Evaluation:** Embed the model in an application and measure improvement
- Example: Test two language models in speech recognition, compare transcription accuracy

**Intrinsic Evaluation:** Measure model quality independently of applications
- Faster and cheaper than extrinsic evaluation

**Training Set vs Test Set:**
- Train model on training set
- Test on unseen test set
- Prevents overfitting to training data

### Perplexity

**Perplexity** is the standard metric for evaluating language models:

```
Perplexity = 2^(-1/N * Î£ logâ‚‚ P(wi | context))

Lower perplexity = better model
Intuition: How "surprised" is the model by the test data?
```

**Example:**
- Model A perplexity = 50 (less surprised)
- Model B perplexity = 200 (more surprised)
- Model A is better

### Cross-Entropy

Cross-entropy measures how well a language model predicts a sequence:

```
Steps:
1. Train N-gram model on training data
2. For each word in test data, compute P(word | context)
3. Take logarithm of each probability and sum
4. Normalize by number of words
```

Lower cross-entropy = better model performance

---

## 3. Smoothing Techniques

### Why Smoothing is Needed

Without smoothing, unseen sequences get probability = 0:

```
Training: "I love coding", "She likes mathematics", "He loves coding"
Test: "I love mathematics"
P("mathematics" | "love") = 0/1 = 0 âŒ
```

### Types of Smoothing

#### Add-1 (Laplace) Smoothing
```
P(wi | wi-1) = (count(wi-1, wi) + 1) / (count(wi-1) + V)

Where V = vocabulary size
```

#### Additive Smoothing
```
P(wi | wi-1) = (count(wi-1, wi) + Î´) / (count(wi-1) + Î´ Ã— V)
```

#### Good-Turing Smoothing
- Use frequency of N-grams that appeared once to estimate probability of unseen N-grams
- Reallocate probability mass from seen to unseen sequences

#### Kneser-Ney Smoothing
- Absolute discounting: subtract constant d from observed N-grams
- Distribute discounted probability to unseen N-grams

#### Katz Smoothing
- Combine Good-Turing with interpolation
- Backoff to lower-order N-grams for unseen sequences

---

## 4. Interpolation and Backoff

### Backoff
```
Strategy:
1. Try N-gram
2. If count = 0 â†’ backoff to (N-1)-gram
3. If still zero â†’ backoff to (N-2)-gram
```

### Interpolation
```
Always mix all N-gram levels:
P = Î»1 Ã— P_unigram + Î»2 Ã— P_bigram + Î»3 Ã— P_trigram

Where Î»1 + Î»2 + Î»3 = 1
```

**Key Difference:**
- Backoff: Use higher-order N-gram when available, fall back when needed
- Interpolation: Always use weighted combination of all levels

---

## 5. Parts-of-Speech (POS) Tagging

### What is POS Tagging?

POS tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word in a sentence.

```
Input:  "The quick brown fox jumps over the lazy dog"
Output: "The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN"
```

### Common POS Tags (Penn Treebank)
- **DT** - Determiner (the, a, an)
- **NN** - Noun, singular (cat, dog)
- **NNS** - Noun, plural (cats, dogs)
- **VB** - Verb, base form (run, eat)
- **VBZ** - Verb, 3rd person singular (runs, eats)
- **JJ** - Adjective (quick, lazy)
- **RB** - Adverb (quickly, very)
- **PRP** - Personal pronoun (I, he, she)

### Three Approaches to POS Tagging

#### 1. Rule-Based Tagging
- Uses dictionary lookup + manually written grammar rules
- Example rules:
  - After "the" â†’ usually noun
  - Words ending in "-ly" â†’ usually adverb
  - After "to" â†’ usually verb

**Advantages:** No training data needed, linguistically correct
**Disadvantages:** Hard to write rules for all cases, time-consuming

#### 2. Statistical (Stochastic) Tagging
- Uses probability and large training data
- Two key probabilities:
  - **Emission:** P(word | tag)
  - **Transition:** P(tag_i | tag_{i-1})
- Common models: HMM, Maximum Entropy

**Advantages:** High accuracy, handles ambiguity well
**Disadvantages:** Needs large tagged corpus, training time

#### 3. Transformation-Based Tagging (Brill Tagger)
- Hybrid approach combining rules and statistics
- Steps:
  1. Assign most frequent tag to each word
  2. Apply transformation rules to correct mistakes
  3. Rules learned from training data

**Example:**
- Initial: "book" â†’ Noun
- Rule: If word follows "to", change to Verb
- Final: "to book" â†’ book = Verb

---

## 6. Hidden Markov Model (HMM)

### Basic Concepts

HMM is a probabilistic model for sequence prediction where states are hidden but outputs are observable.

**Two Key Assumptions:**
1. **Markov Assumption:** Current tag depends only on previous tag
   ```
   P(tag_i | tag_{i-1})
   ```
2. **Emission Independence:** Word depends only on current tag
   ```
   P(word | tag)
   ```

### Components
- **States:** Hidden variables (POS tags)
- **Observations:** Visible outputs (words)
- **Transition Probability:** P(tag_i | tag_{i-1})
- **Emission Probability:** P(word | tag)
- **Initial Probability:** P(first tag)

### Example: "The dog barks"

**Possible tags:** The/DT, dog/NN, barks/NN/VB

**Two possible sequences:**
1. DT â†’ NN â†’ VB
2. DT â†’ NN â†’ NN

**Calculate probabilities:**
```
Sequence 1: P = P(DT) Ã— P(The|DT) Ã— P(NN|DT) Ã— P(dog|NN) Ã— P(VB|NN) Ã— P(barks|VB)
Sequence 2: P = P(DT) Ã— P(The|DT) Ã— P(NN|DT) Ã— P(dog|NN) Ã— P(NN|NN) Ã— P(barks|NN)
```

Choose sequence with higher probability â†’ DT â†’ NN â†’ VB

### Viterbi Algorithm
Efficient algorithm to find the best tag sequence without checking all combinations.

---

## 7. Maximum Entropy Model (MaxEnt)

### How MaxEnt Works

Unlike HMM, MaxEnt considers ALL features of the input, not just previous tag.

**Formula:**
```
P(tag | features) = exp(Î£ Î»áµ¢ Ã— featureáµ¢) / Î£ over all tags
```

**Features can include:**
- Previous word
- Next word
- Word suffix
- Word position
- Capitalization

### Example: "I can swim"

For "can":
- Features: previous word="I", next word="swim"
- Model calculates:
  - P(Modal | context) > P(Noun | context)
  - Result: can = Modal Verb

### Advantages over HMM
- Uses rich feature set
- No Markov assumption limitation
- Better handling of complex patterns

---

## 8. Issues in POS Tagging

### 1. Ambiguity in Word Usage
```
"run" can be:
  - Verb: "I run every day"
  - Noun: "The run was exhausting"
```

### 2. Unknown Words (OOV)
Words not in training data:
```
"I googled the recipe"
"googled" might be tagged incorrectly
```

### 3. Complex Grammar
- Agglutinative languages (Turkish, Finnish)
- Free word order languages (Latin, Sanskrit)

### 4. Tagset Variability
- Penn Treebank: NN, NNS
- Universal Tagset: NOUN

### 5. Idiomatic Expressions
```
"kick the bucket" (meaning "to die")
Literal tags: kick/VB, the/DT, bucket/NN
Actual meaning: verb phrase
```

### 6. Noisy Text
Social media text with misspellings and slang:
```
"OMG, that's sooo cool!"
"OMG" and "sooo" are challenging
```

---

# 2-MARK QUESTIONS (80 words)

---

### Q1. What is an N-gram model?
**Answer:** An N-gram model is a probabilistic language model that predicts the next word in a sequence based on the previous N-1 words. It uses conditional probability calculated from training data: P(word | previous N-1 words) = Count(previous N-1 words + word) / Count(previous N-1 words). For example, bigrams use one previous word, trigrams use two previous words. N-grams are fundamental in statistical NLP for tasks like autocomplete, speech recognition, and machine translation.

---

### Q2. List five limitations of unsmoothed N-gram models.
**Answer:** (1) Data sparsity: unseen word sequences get zero probability. (2) High memory requirements for large N values. (3) Poor performance for rare word sequences. (4) Limited context understanding (only N-1 previous words). (5) Sensitivity to training corpus quality and size. These limitations make unsmoothed models unreliable for real-world applications where unseen sequences are common.

---

### Q3. What is perplexity in NLP?
**Answer:** Perplexity is the standard metric for evaluating language models. It's the inverse probability of the test set, normalized by the number of words: PPL = 2^(-1/N Ã— Î£ logâ‚‚ P(wi | context)). Lower perplexity indicates better model performance. Intuitively, it measures how "surprised" the model is by the test data. A model with perplexity 50 is better than one with perplexity 200.

---

### Q4. What is POS tagging?
**Answer:** Parts-of-Speech (POS) tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word in a sentence. Example: "The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN". POS tagging is crucial for understanding sentence structure, resolving ambiguity, and enabling downstream NLP tasks like parsing, named entity recognition, and machine translation.

---

### Q5. What are the three approaches to POS tagging?
**Answer:** (1) Rule-Based tagging uses dictionary lookup and manually written grammar rules. (2) Statistical (Stochastic) tagging uses probability and large training data with models like HMM and MaxEnt. (3) Transformation-Based tagging (Brill Tagger) combines rule-based and statistical methods, starting with frequent tags and applying learned transformation rules to correct mistakes.

---

### Q6. What is Add-1 (Laplace) smoothing?
**Answer:** Add-1 smoothing adds 1 to all word counts to ensure no zero probabilities. Formula: P(wi | wi-1) = (count(wi-1, wi) + 1) / (count(wi-1) + V), where V is vocabulary size. This simple technique ensures unseen word sequences have non-zero probability, making the model more robust for handling out-of-vocabulary sequences in test data.

---

### Q7. Explain the two key assumptions of HMM.
**Answer:** (1) Markov Assumption: the current tag depends only on the previous tag, not the entire sentence: P(tag_i | tag_{i-1}). (2) Emission Independence Assumption: a word depends only on its current tag: P(word | tag). These assumptions simplify computation while maintaining reasonable accuracy for sequence labeling tasks like POS tagging.

---

### Q8. What is the difference between backoff and interpolation?
**Answer:** Backoff uses N-gram when available, falls back to (N-1)-gram only if count is zero. Interpolation always mixes all N-gram levels with weights: P = Î»1 Ã— P_unigram + Î»2 Ã— P_bigram + Î»3 Ã— P_trigram, where Î»1 + Î»2 + Î»3 = 1. Backoff is hierarchical, interpolation is always combined.

---

### Q9. What is cross-entropy in NLP?
**Answer:** Cross-entropy measures how well a language model predicts a sequence of words. Steps: train model, compute conditional probabilities for test words, take logarithm of each probability and sum, normalize by sequence length. Lower cross-entropy indicates better model performance. It's closely related to perplexity: PPL = 2^cross-entropy.

---

### Q10. Name three issues in POS tagging.
**Answer:** (1) Ambiguity in word usage â€” same word can have different POS tags depending on context (e.g., "run" as verb vs noun). (2) Unknown words â€” out-of-vocabulary words not in training data are hard to tag correctly. (3) Noisy text â€” social media text with misspellings, abbreviations, and slang reduces tagging accuracy.

---

### Q11. What is Good-Turing smoothing?
**Answer:** Good-Turing smoothing reallocates probability distribution using frequency of N-grams. For unseen N-grams, uses frequency of N-grams that appeared once. For known N-grams, uses adjusted counts: c* = (c+1) Ã— (N_{c+1}/N_c), where c is count, N_c is number of N-grams with count c. This technique effectively handles data sparsity.

---

### Q12. What is the Viterbi algorithm used for?
**Answer:** The Viterbi algorithm efficiently finds the most probable sequence of hidden states (POS tags) in an HMM. Instead of checking all possible combinations, it uses dynamic programming to find the best path through the state space. It's essential for HMM-based POS tagging, avoiding exponential computation while finding optimal tag sequences.

---

### Q13. What is Maximum Entropy Model?
**Answer:** Maximum Entropy (MaxEnt) Model is a statistical model that considers all features of input without Markov assumptions. It uses exponential function: P(tag | features) = exp(Î£ Î»áµ¢ Ã— featureáµ¢) / Î£ over all tags. Unlike HMM, MaxEnt can use rich contextual features like word position, surrounding words, suffixes, making it more flexible for complex patterns.

---

### Q14. What is extrinsic vs intrinsic evaluation?
**Answer:** Extrinsic evaluation embeds model in an application and measures improvement (e.g., speech recognition accuracy). Intrinsic evaluation measures model quality independently of applications (e.g., perplexity). Extrinsic is more realistic but expensive; intrinsic is faster for comparing model improvements during development.

---

### Q15. What is Transformation-Based tagging?
**Answer:** Transformation-Based tagging (Brill Tagger) is a hybrid approach combining rules and statistics. Steps: (1) Assign most frequent tag to each word, (2) Apply learned transformation rules to correct mistakes, (3) Rules like "change tag to Verb if word follows 'to'". It achieves good accuracy while maintaining interpretability of correction rules.

---

# 5-MARK QUESTIONS (140 words)

---

### Q1. Explain N-gram models with a complete example including tokenization, construction, and probability calculation.

**Answer:**

An N-gram model predicts the next word based on previous N-1 words using conditional probability. Complete example with "I love programming. I love Python programming":

**Step 1 â€” Tokenization:** ["I", "love", "programming", "I", "love", "Python", "programming"]

**Step 2 â€” Bigram Construction:** ["I love", "love programming", "programming I", "I love", "love Python", "Python programming"]

**Step 3 â€” Frequency Counts:** "I love" appears 2 times, "love programming" 1 time, "love Python" 1 time, "Python programming" 1 time

**Step 4 â€” Probability Calculation:** P("programming" | "love") = Count("love programming") / Count("love") = 1 / (2+1+1) = 0.25. P("I" | "programming") = 1/1 = 1.0. P("Python" | "love") = 1/4 = 0.25.

This model can predict "programming" after "love" with 25% probability, enabling applications like autocomplete and speech recognition.

---

### Q2. Explain smoothing techniques in NLP with examples of Add-1 and Good-Turing smoothing.

**Answer:**

Smoothing adjusts probabilities to handle unseen word sequences, preventing zero probabilities. **Add-1 (Laplace) smoothing** adds 1 to all counts: P(wi | wi-1) = (count(wi-1, wi) + 1) / (count(wi-1) + V), where V is vocabulary size. For training "I love coding", "She likes mathematics", test "I love mathematics": P("mathematics" | "love") = (0+1)/(1+9) = 1/10 instead of 0.

**Good-Turing smoothing** reallocates probability using frequency of N-grams. For unseen bigrams, uses count of bigrams appearing once. For seen bigrams, uses adjusted count: c* = (c+1) Ã— (N_{c+1}/N_c). This technique effectively handles data sparsity by redistributing probability mass from frequent to unseen sequences, improving model robustness for out-of-vocabulary words.

---

### Q3. Explain the three approaches to POS tagging with examples and compare their advantages and disadvantages.

**Answer:**

**Rule-Based tagging** uses dictionary lookup and manually written rules. Example: "play" â†’ Noun/Verb, rule "after 'the' â†’ Noun" tags "The play" correctly. Advantages: no training data, linguistically correct. Disadvantages: hard to write comprehensive rules, time-consuming maintenance.

**Statistical tagging** uses probability from training data. Models like HMM calculate P(word|tag) and P(tag|previous_tag). Advantages: high accuracy, handles ambiguity. Disadvantages: needs large tagged corpus, training time.

**Transformation-Based tagging** (Brill Tagger) combines both approaches. Steps: assign frequent tags, apply learned rules like "change to Verb after 'to'". Example: "to book" â†’ book changes from Noun to Verb. Advantages: good accuracy, interpretable rules. Disadvantages: requires training data, slower training.

Rule-based is transparent but limited; statistical is accurate but data-hungry; transformation-based balances both with learned correction rules.

---

### Q4. Explain Hidden Markov Model for POS tagging with assumptions, components, and a complete example.

**Answer:**

HMM is a probabilistic model for sequence prediction with two key assumptions: **Markov Assumption** (current tag depends only on previous tag: P(tag_i | tag_{i-1})) and **Emission Independence** (word depends only on current tag: P(word | tag)). Components: States (POS tags), Observations (words), Transition Probability, Emission Probability, Initial Probability.

**Example: "The dog barks"**
Possible tags: The/DT, dog/NN, barks/NN/VB

Two sequences:
1. DT â†’ NN â†’ VB: P = P(DT) Ã— P(The|DT) Ã— P(NN|DT) Ã— P(dog|NN) Ã— P(VB|NN) Ã— P(barks|VB)
2. DT â†’ NN â†’ NN: P = P(DT) Ã— P(The|DT) Ã— P(NN|DT) Ã— P(dog|NN) Ã— P(NN|NN) Ã— P(barks|NN)

Using Viterbi algorithm to find best path efficiently. If sequence 1 has higher probability, final tagging: The/DT dog/NN barks/VB.

---

### Q5. Explain interpolation and backoff in N-gram models with examples and compare their approaches.

**Answer:**

**Backoff** uses hierarchical strategy: try N-gram first, if count=0 fall back to (N-1)-gram, continue until non-zero count. Example: for "I love mathematics", if trigram "love mathematics" count=0, backoff to bigram "love" â†’ use P(mathematics|love) if available, else unigram.

**Interpolation** always mixes all N-gram levels with weights: P = Î»1 Ã— P_unigram + Î»2 Ã— P_bigram + Î»3 Ã— P_trigram, where Î»1 + Î»2 + Î»3 = 1. Example: P("mathematics" | "love") = 0.1 Ã— P("mathematics") + 0.3 Ã— P("mathematics" | "love") + 0.6 Ã— P("mathematics" | "I love").

**Comparison:** Backoff uses higher-order models when available, falling back when needed. Interpolation always combines all levels. Backoff is simpler but may miss useful lower-order information; interpolation is more robust but requires weight optimization.

---

### Q6. Explain Maximum Entropy Model and compare it with HMM for POS tagging.

**Answer:**

Maximum Entropy (MaxEnt) Model uses all input features without Markov assumptions. Formula: P(tag | features) = exp(Î£ Î»áµ¢ Ã— featureáµ¢) / Î£ over all tags. Features include previous word, next word, suffix, position, capitalization. Example: for "can", features "previous=I", "next=swim" help determine Modal vs Noun.

**Comparison with HMM:** HMM uses only previous tag (Markov) and current word emission. MaxEnt considers rich contextual features, making it more flexible. HMM learns joint distribution P(Y,X), while MaxEnt learns conditional P(Y|X) directly, better matching the target function. MaxEnt handles complex patterns better but requires more computational resources and feature engineering.

HMM is simpler with strong statistical foundation; MaxEnt is more powerful for complex linguistic patterns but needs careful feature design.

---

### Q7. Explain the major issues in POS tagging with examples and discuss their impact on system performance.

**Answer:**

**Ambiguity:** Words with multiple POS tags. Example: "run" as verb ("I run") vs noun ("The run"). Impact: incorrect tagging affects downstream tasks like parsing.

**Unknown Words:** OOV words not in training data. Example: "googled" in older models. Impact: random or incorrect tag assignment reduces accuracy.

**Complex Grammar:** Agglutinative languages (Turkish) with multiple suffixes, free word order languages (Latin). Impact: requires sophisticated models, increases complexity.

**Tagset Variability:** Different tagsets (Penn vs Universal). Example: NN/NNS vs NOUN. Impact: compatibility issues between systems.

**Noisy Text:** Social media with misspellings and slang. Example: "OMG, sooo cool!". Impact: reduced accuracy on informal text.

**Idiomatic Expressions:** Phrases not following grammar rules. Example: "kick the bucket" (die). Impact: literal tagging misses semantic meaning.

These issues require robust models, large training data, and domain adaptation strategies for accurate POS tagging.

---

### Q8. Explain the evaluation methods for N-gram models including perplexity, cross-entropy, and extrinsic/intrinsic evaluation.

**Answer:**

**Perplexity** measures model quality: PPL = 2^(-1/N Ã— Î£ logâ‚‚ P(wi | context)). Lower perplexity = better model. Intuitively measures how "surprised" the model is by test data.

**Cross-entropy** calculates prediction quality: train model, compute probabilities for test words, take log probabilities, normalize by word count. Lower cross-entropy = better performance.

**Intrinsic Evaluation** measures model quality independently of applications using metrics like perplexity and cross-entropy. Faster and cheaper for comparing model improvements during development.

**Extrinsic Evaluation** embeds model in real applications and measures improvement. Example: compare two language models in speech recognition by measuring transcription accuracy. More realistic but expensive and time-consuming.

**Training/Test Split:** Train on training set, evaluate on unseen test set to prevent overfitting. This separation ensures fair evaluation of generalization capability.

---

### Q9. Explain Transformation-Based tagging with workflow, examples, and comparison with other approaches.

**Answer:**

Transformation-Based tagging (Brill Tagger) combines rule-based and statistical approaches. **Workflow:** (1) Assign most frequent tag to each word as initial tagging. (2) Apply transformation rules learned from training data to correct mistakes. (3) Rules applied iteratively until convergence.

**Examples:** Rule: "If word follows 'to', change tag to Verb". Initial: "book" â†’ Noun. After rule: "to book" â†’ book = Verb. Another rule: "If word follows 'is' and ends with '-ing', change to Verb". Initial: "cooking" â†’ Noun. After rule: "is cooking" â†’ cooking = Verb.

**Comparison:** More accurate than pure rule-based due to learned rules. More interpretable than pure statistical due to explicit correction rules. Requires training data like statistical methods but maintains transparency of rule-based systems. Good balance between accuracy and interpretability, though training can be slower than pure statistical approaches.

---

### Q10. Explain the complete workflow of POS tagging including tokenization, model loading, and result analysis.

**Answer:**

**Workflow Steps:**

1. **Tokenization:** Divide input text into individual words or subwords. Example: "The cat sat" â†’ ["The", "cat", "sat"]

2. **Loading Language Models:** Load pre-trained models (NLTK, SpaCy) trained on large linguistic corpora. These models provide grammatical structure foundation.

3. **Text Processing:** Preprocess text â€” lowercase conversion, handle special characters, remove unnecessary information. Clean text improves tagging accuracy.

4. **Linguistic Analysis:** Determine grammatical structure by analyzing each word's role in the sentence context, considering surrounding words and patterns.

5. **POS Tagging:** Assign grammatical categories using chosen approach (rule-based, statistical, or transformation-based). Output: "The/DT cat/NN sat/VBD"

6. **Results Analysis:** Verify tagging accuracy against source text, identify and correct mistagging. Ensure consistency and proper handling of edge cases.

This systematic workflow ensures accurate POS tagging, essential for downstream NLP tasks like parsing, named entity recognition, and machine translation.
